{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Original Source: https://www.datacamp.com/tutorial/sam2-fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dockn\\SAM-code\\segment-anything-2\\sam2\\modeling\\sam\\transformer.py:23: UserWarning: Flash Attention is disabled as it requires a GPU with Ampere (8.0) CUDA capability.\n",
      "  OLD_GPU, USE_FLASH_ATTN, MATH_KERNEL_ON = get_sdpa_settings()\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import torch\n",
    "import torch.nn.utils\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sam2.build_sam import build_sam2\n",
    "from sam2.sam2_image_predictor import SAM2ImagePredictor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: T-model: 이전 모델 학습시켰던 데이터셋 받기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the chest-ct-segmentation dataset folder\n",
    "data_dir = \"/content/segment-anything-2/chest-ct-segmentation\"\n",
    "images_dir = os.path.join(data_dir, \"images/images\")\n",
    "masks_dir = os.path.join(data_dir, \"masks/masks\")\n",
    "\n",
    "# Load the train.csv file\n",
    "train_df = pd.read_csv(os.path.join(data_dir, \"train.csv\"))\n",
    "\n",
    "# Split the data into two halves: one for training and one for testing\n",
    "train_df, test_df = train_test_split(train_df, test_size=0.2, random_state=42)\n",
    "\n",
    "# Prepare the training data list\n",
    "train_data = []\n",
    "for index, row in train_df.iterrows():\n",
    "   image_name = row['ImageId']\n",
    "   mask_name = row['MaskId']\n",
    "\n",
    "   # Append image and corresponding mask paths\n",
    "   train_data.append({\n",
    "       \"image\": os.path.join(images_dir, image_name),\n",
    "       \"annotation\": os.path.join(masks_dir, mask_name)\n",
    "   })\n",
    "\n",
    "# Prepare the testing data list (if needed for inference or evaluation later)\n",
    "test_data = []\n",
    "for index, row in test_df.iterrows():\n",
    "   image_name = row['ImageId']\n",
    "   mask_name = row['MaskId']\n",
    "\n",
    "   # Append image and corresponding mask paths\n",
    "   test_data.append({\n",
    "       \"image\": os.path.join(images_dir, image_name),\n",
    "       \"annotation\": os.path.join(masks_dir, mask_name)\n",
    "   })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_batch(data, visualize_data=False):\n",
    "    # Select a random entry\n",
    "    ent = data[np.random.randint(len(data))]\n",
    "\n",
    "    # Get full paths\n",
    "    Img = cv2.imread(ent[\"image\"])[..., ::-1]  # Convert BGR to RGB\n",
    "    ann_map = cv2.imread(ent[\"annotation\"], cv2.IMREAD_GRAYSCALE)  # Read annotation as grayscale\n",
    "\n",
    "    if Img is None or ann_map is None:\n",
    "        print(f\"Error: Could not read image or mask from path {ent['image']} or {ent['annotation']}\")\n",
    "        return None, None, None, 0\n",
    "\n",
    "    # Resize image and mask\n",
    "    r = np.min([1024 / Img.shape[1], 1024 / Img.shape[0]])  # Scaling factor\n",
    "    Img = cv2.resize(Img, (int(Img.shape[1] * r), int(Img.shape[0] * r)))\n",
    "    ann_map = cv2.resize(ann_map, (int(ann_map.shape[1] * r), int(ann_map.shape[0] * r)), interpolation=cv2.INTER_NEAREST)\n",
    "\n",
    "    # Initialize a single binary mask\n",
    "    binary_mask = np.zeros_like(ann_map, dtype=np.uint8)\n",
    "    points = []\n",
    "\n",
    "    # Get binary masks and combine them into a single mask\n",
    "    inds = np.unique(ann_map)[1:]  # Skip the background (index 0)\n",
    "    for ind in inds:\n",
    "        mask = (ann_map == ind).astype(np.uint8)  # Create binary mask for each unique index\n",
    "        binary_mask = np.maximum(binary_mask, mask)  # Combine with the existing binary mask\n",
    "\n",
    "    # Erode the combined binary mask to avoid boundary points\n",
    "    eroded_mask = cv2.erode(binary_mask, np.ones((5, 5), np.uint8), iterations=1)\n",
    "\n",
    "    # Get all coordinates inside the eroded mask and choose a random point\n",
    "    coords = np.argwhere(eroded_mask > 0)\n",
    "    if len(coords) > 0:\n",
    "        for _ in inds:  # Select as many points as there are unique labels\n",
    "            yx = np.array(coords[np.random.randint(len(coords))])\n",
    "            points.append([yx[1], yx[0]])\n",
    "\n",
    "    points = np.array(points)\n",
    "    ### Continuation of read_batch() ###\n",
    "\n",
    "    if visualize_data:\n",
    "        # Plotting the images and points\n",
    "        plt.figure(figsize=(15, 5))\n",
    "\n",
    "        # Original Image\n",
    "        plt.subplot(1, 3, 1)\n",
    "        plt.title('Original Image')\n",
    "        plt.imshow(Img)\n",
    "        plt.axis('off')\n",
    "\n",
    "        # Segmentation Mask (binary_mask)\n",
    "        plt.subplot(1, 3, 2)\n",
    "        plt.title('Binarized Mask')\n",
    "        plt.imshow(binary_mask, cmap='gray')\n",
    "        plt.axis('off')\n",
    "\n",
    "        # Mask with Points in Different Colors\n",
    "        plt.subplot(1, 3, 3)\n",
    "        plt.title('Binarized Mask with Points')\n",
    "        plt.imshow(binary_mask, cmap='gray')\n",
    "\n",
    "        # Plot points in different colors\n",
    "        colors = list(mcolors.TABLEAU_COLORS.values())\n",
    "        for i, point in enumerate(points):\n",
    "            plt.scatter(point[0], point[1], c=colors[i % len(colors)], s=100, label=f'Point {i+1}')  # Corrected to plot y, x order\n",
    "\n",
    "        # plt.legend()\n",
    "        plt.axis('off')\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    binary_mask = np.expand_dims(binary_mask, axis=-1)  # Now shape is (1024, 1024, 1)\n",
    "    binary_mask = binary_mask.transpose((2, 0, 1))\n",
    "    points = np.expand_dims(points, axis=1)\n",
    "\n",
    "    # Return the image, binarized mask, points, and number of masks\n",
    "    return Img, binary_mask, points, len(inds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the data\n",
    "Img1, masks1, points1, num_masks = read_batch(train_data, visualize_data=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sam2_checkpoint = \"sam2_hiera_small.pt\"  # @param [\"sam2_hiera_tiny.pt\", \"sam2_hiera_small.pt\", \"sam2_hiera_base_plus.pt\", \"sam2_hiera_large.pt\"]\n",
    "model_cfg = \"sam2_hiera_s.yaml\" # @param [\"sam2_hiera_t.yaml\", \"sam2_hiera_s.yaml\", \"sam2_hiera_b+.yaml\", \"sam2_hiera_l.yaml\"]\n",
    "\n",
    "sam2_model = build_sam2(model_cfg, sam2_checkpoint, device=\"cuda\")\n",
    "predictor = SAM2ImagePredictor(sam2_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train mask decoder.\n",
    "predictor.model.sam_mask_decoder.train(True)\n",
    "\n",
    "# Train prompt encoder.\n",
    "predictor.model.sam_prompt_encoder.train(True)\n",
    "\n",
    "# Configure optimizer.\n",
    "optimizer=torch.optim.AdamW(params=predictor.model.parameters(),lr=0.0001,weight_decay=1e-4) #1e-5, weight_decay = 4e-5\n",
    "\n",
    "# Mix precision.\n",
    "scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "# No. of steps to train the model.\n",
    "NO_OF_STEPS = 3000 # @param \n",
    "\n",
    "# Fine-tuned model name.\n",
    "FINE_TUNED_MODEL_NAME = \"fine_tuned_sam2\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize scheduler\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=500, gamma=0.2) # 500 , 250, gamma = 0.1\n",
    "accumulation_steps = 4  # Number of steps to accumulate gradients before updating\n",
    "\n",
    "for step in range(1, NO_OF_STEPS + 1):\n",
    "   with torch.cuda.amp.autocast():\n",
    "   # with torch.autocast(\"cuda\"):\n",
    "\n",
    "       image, mask, input_point, num_masks = read_batch(train_data, visualize_data=False)\n",
    "       if image is None or mask is None or num_masks == 0:\n",
    "           continue\n",
    "\n",
    "       input_label = np.ones((num_masks, 1))\n",
    "       if not isinstance(input_point, np.ndarray) or not isinstance(input_label, np.ndarray):\n",
    "           continue\n",
    "\n",
    "       if input_point.size == 0 or input_label.size == 0:\n",
    "           continue\n",
    "\n",
    "       predictor.set_image(image)\n",
    "       mask_input, unnorm_coords, labels, unnorm_box = predictor._prep_prompts(input_point, input_label, box=None, mask_logits=None, normalize_coords=True)\n",
    "       if unnorm_coords is None or labels is None or unnorm_coords.shape[0] == 0 or labels.shape[0] == 0:\n",
    "           continue\n",
    "\n",
    "       sparse_embeddings, dense_embeddings = predictor.model.sam_prompt_encoder(\n",
    "           points=(unnorm_coords, labels), boxes=None, masks=None,\n",
    "       )\n",
    "\n",
    "       batched_mode = unnorm_coords.shape[0] > 1\n",
    "       high_res_features = [feat_level[-1].unsqueeze(0) for feat_level in predictor._features[\"high_res_feats\"]]\n",
    "       low_res_masks, prd_scores, _, _ = predictor.model.sam_mask_decoder(\n",
    "           image_embeddings=predictor._features[\"image_embed\"][-1].unsqueeze(0),\n",
    "           image_pe=predictor.model.sam_prompt_encoder.get_dense_pe(),\n",
    "           sparse_prompt_embeddings=sparse_embeddings,\n",
    "           dense_prompt_embeddings=dense_embeddings,\n",
    "           multimask_output=True,\n",
    "           repeat_image=batched_mode,\n",
    "           high_res_features=high_res_features,\n",
    "       )\n",
    "       prd_masks = predictor._transforms.postprocess_masks(low_res_masks, predictor._orig_hw[-1])\n",
    "\n",
    "       gt_mask = torch.tensor(mask.astype(np.float32)).cuda()\n",
    "       prd_mask = torch.sigmoid(prd_masks[:, 0])\n",
    "       seg_loss = (-gt_mask * torch.log(prd_mask + 0.000001) - (1 - gt_mask) * torch.log((1 - prd_mask) + 0.00001)).mean()\n",
    "\n",
    "       inter = (gt_mask * (prd_mask > 0.5)).sum(1).sum(1)\n",
    "       iou = inter / (gt_mask.sum(1).sum(1) + (prd_mask > 0.5).sum(1).sum(1) - inter)\n",
    "       score_loss = torch.abs(prd_scores[:, 0] - iou).mean()\n",
    "       loss = seg_loss + score_loss * 0.05\n",
    "\n",
    "       # Apply gradient accumulation\n",
    "       loss = loss / accumulation_steps\n",
    "       scaler.scale(loss).backward()\n",
    "\n",
    "       # Clip gradients\n",
    "       torch.nn.utils.clip_grad_norm_(predictor.model.parameters(), max_norm=1.0)\n",
    "\n",
    "       if step % accumulation_steps == 0:\n",
    "           scaler.step(optimizer)\n",
    "           scaler.update()\n",
    "           predictor.model.zero_grad()\n",
    "\n",
    "       # Update scheduler\n",
    "       scheduler.step()\n",
    "\n",
    "       if step % 500 == 0:\n",
    "           FINE_TUNED_MODEL = FINE_TUNED_MODEL_NAME + \"_\" + str(step) + \".torch\"\n",
    "           torch.save(predictor.model.state_dict(), FINE_TUNED_MODEL)\n",
    "\n",
    "       if step == 1:\n",
    "           mean_iou = 0\n",
    "\n",
    "       mean_iou = mean_iou * 0.99 + 0.01 * np.mean(iou.cpu().detach().numpy())\n",
    "\n",
    "       if step % 100 == 0:\n",
    "           print(\"Step \" + str(step) + \":\\t\", \"Accuracy (IoU) = \", mean_iou)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_image(image_path, mask_path):  # read and resize image and mask\n",
    "   img = cv2.imread(image_path)[..., ::-1]  # Convert BGR to RGB\n",
    "   mask = cv2.imread(mask_path, 0)\n",
    "   r = np.min([1024 / img.shape[1], 1024 / img.shape[0]])\n",
    "   img = cv2.resize(img, (int(img.shape[1] * r), int(img.shape[0] * r)))\n",
    "   mask = cv2.resize(mask, (int(mask.shape[1] * r), int(mask.shape[0] * r)), interpolation=cv2.INTER_NEAREST)\n",
    "   return img, mask\n",
    "\n",
    "def get_points(mask, num_points):  # Sample points inside the input mask\n",
    "   points = []\n",
    "   coords = np.argwhere(mask > 0)\n",
    "   for i in range(num_points):\n",
    "       yx = np.array(coords[np.random.randint(len(coords))])\n",
    "       points.append([[yx[1], yx[0]]])\n",
    "   return np.array(points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomly select a test image from the test_data\n",
    "selected_entry = random.choice(test_data)\n",
    "image_path = selected_entry['image']\n",
    "mask_path = selected_entry['annotation']\n",
    "\n",
    "# Load the selected image and mask\n",
    "image, mask = read_image(image_path, mask_path)\n",
    "\n",
    "# Generate random points for the input\n",
    "num_samples = 30  # Number of points per segment to sample\n",
    "input_points = get_points(mask, num_samples)\n",
    "\n",
    "# Load the fine-tuned model\n",
    "FINE_TUNED_MODEL_WEIGHTS = \"fine_tuned_sam2_1000.torch\"\n",
    "sam2_model = build_sam2(model_cfg, sam2_checkpoint, device=\"cuda\")\n",
    "\n",
    "# Build net and load weights\n",
    "predictor = SAM2ImagePredictor(sam2_model)\n",
    "predictor.model.load_state_dict(torch.load(FINE_TUNED_MODEL_WEIGHTS))\n",
    "\n",
    "# Perform inference and predict masks\n",
    "with torch.no_grad():\n",
    "   predictor.set_image(image)\n",
    "   masks, scores, logits = predictor.predict(\n",
    "       point_coords=input_points,\n",
    "       point_labels=np.ones([input_points.shape[0], 1])\n",
    "   )\n",
    "\n",
    "# Process the predicted masks and sort by scores\n",
    "np_masks = np.array(masks[:, 0])\n",
    "np_scores = scores[:, 0]\n",
    "sorted_masks = np_masks[np.argsort(np_scores)][::-1]\n",
    "\n",
    "# Initialize segmentation map and occupancy mask\n",
    "seg_map = np.zeros_like(sorted_masks[0], dtype=np.uint8)\n",
    "occupancy_mask = np.zeros_like(sorted_masks[0], dtype=bool)\n",
    "\n",
    "# Combine masks to create the final segmentation map\n",
    "for i in range(sorted_masks.shape[0]):\n",
    "   mask = sorted_masks[i]\n",
    "   if (mask * occupancy_mask).sum() / mask.sum() > 0.15:\n",
    "       continue\n",
    "\n",
    "   mask_bool = mask.astype(bool)\n",
    "   mask_bool[occupancy_mask] = False  # Set overlapping areas to False in the mask\n",
    "   seg_map[mask_bool] = i + 1  # Use boolean mask to index seg_map\n",
    "   occupancy_mask[mask_bool] = True  # Update occupancy_mask\n",
    "\n",
    "# Visualization: Show the original image, mask, and final segmentation side by side\n",
    "plt.figure(figsize=(18, 6))\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.title('Test Image')\n",
    "plt.imshow(image)\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.title('Original Mask')\n",
    "plt.imshow(mask, cmap='gray')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.title('Final Segmentation')\n",
    "plt.imshow(seg_map, cmap='jet')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "JY",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
