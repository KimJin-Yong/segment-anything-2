{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# object projection using depth map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.ndimage import gaussian_filter\n",
    "\n",
    "def smooth_coordinates(new_xs, new_ys, sigma=1):\n",
    "    \"\"\"\n",
    "    좌표값에 가우시안 필터를 적용하여 스무딩.\n",
    "    \"\"\"\n",
    "    smoothed_xs = gaussian_filter(new_xs.astype(float), sigma=sigma)\n",
    "    smoothed_ys = gaussian_filter(new_ys.astype(float), sigma=sigma)\n",
    "    return smoothed_xs.astype(int), smoothed_ys.astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_median_filter(frame):\n",
    "    \"\"\"\n",
    "    색 채널이 (0, 0, 0)인 부분에 평균 필터를 적용합니다.\n",
    "    \"\"\"\n",
    "    # 검은색 픽셀을 마스크로 생성 (RGB가 모두 0인 경우)\n",
    "    mask = np.all(frame == [0, 0, 0], axis=-1)\n",
    "\n",
    "    # 마스크 영역을 제외한 부분에 평균 필터 적용\n",
    "    filtered_frame = cv2.medianBlur(frame, 7)\n",
    "\n",
    "    # 원본 프레임의 마스크된 부분은 그대로 유지하고, 나머지 부분에 필터링된 결과를 적용\n",
    "    output_frame = np.copy(frame)\n",
    "    output_frame[mask] = filtered_frame[mask]\n",
    "\n",
    "    return output_frame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def orthographic_projection(input_video_path, depth_map_tensor, mask_tensor, output_video_path, focal_length):\n",
    "    cap = cv2.VideoCapture(input_video_path)\n",
    "\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "    out = cv2.VideoWriter(output_video_path, fourcc, fps, (width, height))\n",
    "\n",
    "    # 첫 번째 프레임 처리\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        return\n",
    "\n",
    "    # 마스크를 적용하여 깊이 맵 추출\n",
    "    masked_depth = depth_map_tensor * mask_tensor\n",
    "\n",
    "    # 마스크된 픽셀 인덱스 가져오기\n",
    "    mask_indices = np.argwhere(mask_tensor.numpy())\n",
    "    depth_values = masked_depth[mask_indices[:, 0], mask_indices[:, 1]].cpu().numpy()  # Ensure to move to CPU\n",
    "\n",
    "    # 깊이 값의 최소, 최대 범위 정의 (0으로 나누는 경우 방지)\n",
    "    min_depth = np.min(depth_values)\n",
    "    max_depth = np.max(depth_values)\n",
    "\n",
    "    # 깊이에 따른 스케일링 적용 (깊이에 반비례하여 적용)\n",
    "    scale_factor = (max_depth - depth_values) / (max_depth - min_depth + 1e-6)\n",
    "\n",
    "    # z축 정사영 변환 적용\n",
    "    new_xs = (mask_indices[:, 1] + (depth_values * scale_factor)).astype(int)\n",
    "    new_ys = (mask_indices[:, 0] + (depth_values * scale_factor)).astype(int)\n",
    "\n",
    "    # # 좌표 스무딩 적용\n",
    "    # new_xs, new_ys = smooth_coordinates(new_xs, new_ys, sigma=1)\n",
    "\n",
    "    # 유효한 좌표 필터링\n",
    "    valid_mask = (0 <= new_xs) & (new_xs < width) & (0 <= new_ys) & (new_ys < height)\n",
    "\n",
    "    # 비디오 프레임에 새로운 픽셀 값 할당\n",
    "    if np.any(valid_mask):  # valid_mask가 True인 경우만\n",
    "        frame[new_ys[valid_mask], new_xs[valid_mask]] = frame[mask_indices[valid_mask][:, 0], mask_indices[valid_mask][:, 1]]\n",
    "        \n",
    "    # 정사영된 좌표 외의 영역을 검은색으로 설정\n",
    "    valid_projected_mask = np.zeros((height, width), dtype=bool)\n",
    "    valid_projected_mask[new_ys[valid_mask], new_xs[valid_mask]] = True\n",
    "    frame[~valid_projected_mask] = [0, 0, 0]  # 정사영된 좌표 외부를 검은색으로 설정\n",
    "    \n",
    "    smoothed_frame = apply_median_filter(frame)\n",
    "\n",
    "    out.write(smoothed_frame)\n",
    "\n",
    "    # 나머지 프레임 처리\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        # 깊이에 따른 스케일링 적용\n",
    "        scale_factor = (max_depth - depth_values) / (max_depth - min_depth + 1e-6)\n",
    "\n",
    "        # z축 정사영 변환 적용\n",
    "        new_xs = (mask_indices[:, 1] + (depth_values * scale_factor)).astype(int)\n",
    "        new_ys = (mask_indices[:, 0] + (depth_values * scale_factor)).astype(int)\n",
    "\n",
    "        # # 좌표 스무딩 적용\n",
    "        # new_xs, new_ys = smooth_coordinates(new_xs, new_ys, sigma=1)\n",
    "\n",
    "        # 유효한 좌표 필터링\n",
    "        valid_mask = (0 <= new_xs) & (new_xs < width) & (0 <= new_ys) & (new_ys < height)\n",
    "        if np.any(valid_mask):  # valid_mask가 True인 경우만\n",
    "            frame[new_ys[valid_mask], new_xs[valid_mask]] = frame[mask_indices[valid_mask][:, 0], mask_indices[valid_mask][:, 1]]\n",
    "\n",
    "        # 정사영된 좌표 외의 영역을 검은색으로 설정\n",
    "        valid_projected_mask = np.zeros((height, width), dtype=bool)\n",
    "        valid_projected_mask[new_ys[valid_mask], new_xs[valid_mask]] = True\n",
    "        frame[~valid_projected_mask] = [0, 0, 0]  # 정사영된 좌표 외부를 검은색으로 설정\n",
    "\n",
    "        smoothed_frame = apply_median_filter(frame)\n",
    "        out.write(smoothed_frame)\n",
    "\n",
    "    cap.release()\n",
    "    out.release()\n",
    "    cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dockn\\AppData\\Local\\Temp\\ipykernel_40976\\1442653228.py:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  depth_map_tensor = torch.load(r\"C:\\Users\\dockn\\SAM-code\\나주대교_depth.pt\")\n",
      "C:\\Users\\dockn\\AppData\\Local\\Temp\\ipykernel_40976\\1442653228.py:5: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  focal_length = torch.load(r\"C:\\Users\\dockn\\SAM-code\\나주대교_focal.pt\")\n"
     ]
    }
   ],
   "source": [
    "input_video_path = r\"C:\\Users\\dockn\\Downloads\\나주대교.mp4\"    # Path to the 2D input video\n",
    "output_video_path = \"나주대교.mp4\"    # Path to save the output video\n",
    "\n",
    "depth_map_tensor = torch.load(r\"C:\\Users\\dockn\\SAM-code\\나주대교_depth.pt\")\n",
    "focal_length = torch.load(r\"C:\\Users\\dockn\\SAM-code\\나주대교_focal.pt\")\n",
    "\n",
    "mask = plt.imread(r\"C:\\Users\\dockn\\OneDrive\\바탕 화면\\나주.png\")\n",
    "binary = np.logical_not(mask[:,:,-1] == 0)\n",
    "mask_tensor = torch.from_numpy(binary)\n",
    "\n",
    "orthographic_projection(input_video_path, depth_map_tensor, mask_tensor, output_video_path, focal_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dockn\\AppData\\Local\\Temp\\ipykernel_13688\\1207220125.py:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  depth_map_tensor = torch.load(r\"C:\\Users\\dockn\\SAM-code\\삼례_depth.pt\")\n",
      "C:\\Users\\dockn\\AppData\\Local\\Temp\\ipykernel_13688\\1207220125.py:10: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  focal_length = torch.load(r'C:\\Users\\dockn\\SAM-code\\삼례_focal.pt')\n"
     ]
    }
   ],
   "source": [
    "input_video_path = r\"C:\\Users\\dockn\\Downloads\\test.mp4\"  # Path to the 2D input video\n",
    "output_video_path = \"삼례_projection_z.mp4\"  # Path to save the output video\n",
    "\n",
    "depth_map_tensor = torch.load(r\"C:\\Users\\dockn\\SAM-code\\삼례_depth.pt\")\n",
    "mask = plt.imread(r\"C:\\Users\\dockn\\OneDrive\\바탕 화면\\삼례_mask.png\")\n",
    "binary = np.logical_not(mask[:, :, -1] == 0)\n",
    "mask_tensor = torch.from_numpy(binary)\n",
    "\n",
    "# 사용자가 알고 있는 초점거리\n",
    "focal_length = torch.load(r'C:\\Users\\dockn\\SAM-code\\삼례_focal.pt')\n",
    "\n",
    "orthographic_projection(input_video_path, depth_map_tensor, mask_tensor, output_video_path, focal_length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def orthographic_projection_image(input_image_path, depth_map_tensor, mask_tensor, output_image_path, focal_length):\n",
    "    # 이미지 읽기\n",
    "    frame = cv2.imread(input_image_path)\n",
    "\n",
    "    height, width, _ = frame.shape\n",
    "\n",
    "    # 마스크를 적용하여 깊이 맵 추출\n",
    "    masked_depth = depth_map_tensor * mask_tensor\n",
    "\n",
    "    # 마스크된 픽셀 인덱스 가져오기\n",
    "    mask_indices = np.argwhere(mask_tensor.numpy())\n",
    "    depth_values = masked_depth[mask_indices[:, 0], mask_indices[:, 1]].cpu().numpy()  # Ensure to move to CPU\n",
    "\n",
    "    # 깊이 값의 최소, 최대 범위 정의 (0으로 나누는 경우 방지)\n",
    "    min_depth = np.min(depth_values)\n",
    "    max_depth = np.max(depth_values)\n",
    "\n",
    "    # 깊이에 따른 스케일링 적용 (깊이에 반비례하여 적용)\n",
    "    scale_factor = (max_depth - depth_values) / (max_depth - min_depth + 1e-6)\n",
    "\n",
    "    # z축 정사영 변환 적용\n",
    "    new_xs = (mask_indices[:, 1] + (depth_values * scale_factor)).astype(int)\n",
    "    new_ys = (mask_indices[:, 0] + (depth_values * scale_factor)).astype(int)\n",
    "\n",
    "    # 유효한 좌표 필터링\n",
    "    valid_mask = (0 <= new_xs) & (new_xs < width) & (0 <= new_ys) & (new_ys < height)\n",
    "\n",
    "    # 이미지에 새로운 픽셀 값 할당\n",
    "    if np.any(valid_mask):  # valid_mask가 True인 경우만\n",
    "        frame[new_ys[valid_mask], new_xs[valid_mask]] = frame[mask_indices[valid_mask][:, 0], mask_indices[valid_mask][:, 1]]\n",
    "\n",
    "    # 정사영된 좌표 외의 영역을 검은색으로 설정\n",
    "    valid_projected_mask = np.zeros((height, width), dtype=bool)\n",
    "    valid_projected_mask[new_ys[valid_mask], new_xs[valid_mask]] = True\n",
    "    frame[~valid_projected_mask] = [0, 0, 0]  # 정사영된 좌표 외부를 검은색으로 설정\n",
    "\n",
    "    # 중앙값 필터 적용\n",
    "    smoothed_frame = apply_median_filter(frame)\n",
    "\n",
    "    # 처리된 이미지 저장\n",
    "    cv2.imwrite(output_image_path, smoothed_frame)\n",
    "\n",
    "def apply_median_filter(image):\n",
    "    # 중앙값 블러 필터 적용\n",
    "    return cv2.medianBlur(image, 5)\n",
    "\n",
    "# 사용 예시\n",
    "input_image_path = 'input_image.jpg'\n",
    "output_image_path = 'output_image.jpg'\n",
    "# depth_map_tensor, mask_tensor는 미리 정의되어야 합니다.\n",
    "# focal_length는 초점 거리를 사용자가 설정합니다.\n",
    "\n",
    "# 함수 호출\n",
    "orthographic_projection_image(input_image_path, depth_map_tensor, mask_tensor, output_image_path, focal_length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "JY",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
